#!/usr/bin/env python3\n\"\"\"\nAPEX AI DYNAMIC RULES ENGINE INTEGRATION TEST\n=============================================\nComprehensive test suite for the dynamic rules engine integration\nwith geofencing and master threat detection coordinator\n\nTests:\n- Geofencing manager zone creation and detection\n- Dynamic rules engine rule evaluation\n- Master threat coordinator integration\n- Rules configuration management\n- End-to-end threat processing with rules\n\"\"\"\n\nimport sys\nimport os\nimport numpy as np\nimport logging\nfrom datetime import datetime\nimport json\n\n# Add the apex_ai_engine directory to the path\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\n\ntry:\n    # Import geofencing components\n    from geofencing.geofencing_manager import (\n        GeofencingManager, GeofenceZone, ZoneType, CoordinateSystem\n    )\n    \n    # Import dynamic rules engine\n    from dynamic_rules_engine import (\n        DynamicRulesEngine, SecurityRule, RuleCondition, \n        RuleAction, RuleConditionType, RuleOperator\n    )\n    \n    # Import configuration manager\n    from rules_configuration import RulesConfigurationManager\n    \n    # Import master threat coordinator\n    from models.master_threat_coordinator import MasterThreatDetectionCoordinator\n    \n    COMPONENTS_AVAILABLE = True\n    \nexcept ImportError as e:\n    print(f\"❌ Import error: {e}\")\n    print(\"⚠️ Some components may not be available\")\n    COMPONENTS_AVAILABLE = False\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\nclass DynamicRulesEngineIntegrationTest:\n    \"\"\"\n    Comprehensive integration test suite for dynamic rules engine\n    \"\"\"\n    \n    def __init__(self):\n        self.test_results = {\n            'total_tests': 0,\n            'passed_tests': 0,\n            'failed_tests': 0,\n            'test_details': []\n        }\n        \n        # Test components\n        self.geofencing_manager = None\n        self.rules_engine = None\n        self.config_manager = None\n        self.master_coordinator = None\n    \n    def log_test_result(self, test_name: str, passed: bool, details: str = \"\"):\n        \"\"\"Log test result\"\"\"\n        self.test_results['total_tests'] += 1\n        \n        if passed:\n            self.test_results['passed_tests'] += 1\n            status = \"✅ PASSED\"\n        else:\n            self.test_results['failed_tests'] += 1\n            status = \"❌ FAILED\"\n        \n        result_entry = {\n            'test_name': test_name,\n            'status': status,\n            'details': details,\n            'timestamp': datetime.now().isoformat()\n        }\n        \n        self.test_results['test_details'].append(result_entry)\n        logger.info(f\"{status}: {test_name} - {details}\")\n    \n    def test_geofencing_manager_creation(self) -> bool:\n        \"\"\"Test geofencing manager creation and basic operations\"\"\"\n        try:\n            # Create geofencing manager\n            self.geofencing_manager = GeofencingManager()\n            \n            # Test zone creation\n            zone_data = {\n                'zone_id': 'test_entrance',\n                'name': 'Main Entrance Test Zone',\n                'polygon_points': [(0.0, 0.0), (0.5, 0.0), (0.5, 0.3), (0.0, 0.3)],\n                'zone_type': ZoneType.ENTRY_EXIT,\n                'coordinate_system': CoordinateSystem.NORMALIZED,\n                'camera_id': 'CAM-TEST-01',\n                'monitor_id': 'MONITOR-TEST-1'\n            }\n            \n            zone = self.geofencing_manager.create_zone(zone_data)\n            \n            # Test point-in-zone detection\n            point_inside = (0.25, 0.15)  # Should be inside\n            point_outside = (0.75, 0.15)  # Should be outside\n            \n            inside_result = self.geofencing_manager.is_point_in_zone(point_inside, zone.zone_id)\n            outside_result = self.geofencing_manager.is_point_in_zone(point_outside, zone.zone_id)\n            \n            success = inside_result and not outside_result\n            details = f\"Zone created with ID {zone.zone_id}, point detection: inside={inside_result}, outside={outside_result}\"\n            \n            self.log_test_result(\"Geofencing Manager Creation\", success, details)\n            return success\n            \n        except Exception as e:\n            self.log_test_result(\"Geofencing Manager Creation\", False, f\"Exception: {str(e)}\")\n            return False\n    \n    def test_rules_engine_creation(self) -> bool:\n        \"\"\"Test dynamic rules engine creation and rule evaluation\"\"\"\n        try:\n            if not self.geofencing_manager:\n                self.log_test_result(\"Rules Engine Creation\", False, \"Geofencing manager not available\")\n                return False\n            \n            # Create rules engine\n            self.rules_engine = DynamicRulesEngine(self.geofencing_manager)\n            \n            # Create test rule\n            rule_data = {\n                'rule_id': 'test_weapon_alert',\n                'name': 'Test Weapon Detection Rule',\n                'description': 'Alert on weapon detection in test zone',\n                'zone_ids': ['test_entrance'],\n                'conditions': [\n                    {\n                        'condition_id': 'weapon_detected',\n                        'condition_type': 'object_type',\n                        'parameters': {\n                            'forbidden_types': ['weapon', 'gun'],\n                            'min_confidence': 0.8\n                        },\n                        'operator': 'and',\n                        'weight': 1.0\n                    }\n                ],\n                'actions': ['alert', 'record', 'escalate'],\n                'is_active': True,\n                'priority': 10,\n                'confidence_threshold': 0.8\n            }\n            \n            rule = self.rules_engine.create_rule(rule_data)\n            \n            # Test rule evaluation with simulated threat\n            threat_data = {\n                'zone_id': 'test_entrance',\n                'threat_type': 'weapon_detection',\n                'confidence': 85,\n                'bbox': (100, 100, 50, 50),\n                'camera_id': 'CAM-TEST-01'\n            }\n            \n            evaluations = self.rules_engine.evaluate_threat_against_rules(threat_data)\n            \n            success = len(evaluations) > 0 and any(eval.triggered for eval in evaluations)\n            details = f\"Rule created with ID {rule.rule_id}, evaluations: {len(evaluations)}, triggered: {any(eval.triggered for eval in evaluations)}\"\n            \n            self.log_test_result(\"Rules Engine Creation\", success, details)\n            return success\n            \n        except Exception as e:\n            self.log_test_result(\"Rules Engine Creation\", False, f\"Exception: {str(e)}\")\n            return False\n    \n    def test_configuration_manager(self) -> bool:\n        \"\"\"Test rules configuration manager\"\"\"\n        try:\n            if not self.geofencing_manager or not self.rules_engine:\n                self.log_test_result(\"Configuration Manager\", False, \"Prerequisites not available\")\n                return False\n            \n            # Create configuration manager\n            self.config_manager = RulesConfigurationManager()\n            \n            # Test configuration export\n            config_data = {\n                'rules': {'rules': [rule.to_dict() for rule in self.rules_engine.rules.values()]},\n                'zones': {'zones': [zone.to_dict() for zone in self.geofencing_manager.zones.values()]}\n            }\n            \n            # Test configuration validation\n            validation_result = self.config_manager.validate_complete_configuration(config_data)\n            \n            success = validation_result['is_valid']\n            details = f\"Configuration validation: {validation_result['is_valid']}, errors: {len(validation_result['errors'])}\"\n            \n            self.log_test_result(\"Configuration Manager\", success, details)\n            return success\n            \n        except Exception as e:\n            self.log_test_result(\"Configuration Manager\", False, f\"Exception: {str(e)}\")\n            return False\n    \n    def test_master_coordinator_integration(self) -> bool:\n        \"\"\"Test master threat coordinator integration with rules engine\"\"\"\n        try:\n            if not self.geofencing_manager or not self.rules_engine:\n                self.log_test_result(\"Master Coordinator Integration\", False, \"Prerequisites not available\")\n                return False\n            \n            # Create master coordinator with rules engine integration\n            self.master_coordinator = MasterThreatDetectionCoordinator(\n                config={},\n                geofencing_manager=self.geofencing_manager,\n                rules_engine=self.rules_engine,\n                config_manager=self.config_manager\n            )\n            \n            # Test integration status\n            stats = self.master_coordinator.get_processing_statistics()\n            \n            rules_enabled = stats.get('rules_engine', {}).get('rules_engine_enabled', False)\n            geofencing_enabled = stats.get('rules_engine', {}).get('geofencing_enabled', False)\n            \n            success = rules_enabled and geofencing_enabled\n            details = f\"Rules enabled: {rules_enabled}, Geofencing enabled: {geofencing_enabled}\"\n            \n            self.log_test_result(\"Master Coordinator Integration\", success, details)\n            return success\n            \n        except Exception as e:\n            self.log_test_result(\"Master Coordinator Integration\", False, f\"Exception: {str(e)}\")\n            return False\n    \n    def test_end_to_end_threat_processing(self) -> bool:\n        \"\"\"Test end-to-end threat processing with rules evaluation\"\"\"\n        try:\n            if not self.master_coordinator:\n                self.log_test_result(\"End-to-End Processing\", False, \"Master coordinator not available\")\n                return False\n            \n            # Create a simple test frame (black image)\n            test_frame = np.zeros((480, 640, 3), dtype=np.uint8)\n            \n            # Simulate threat detection (this would normally come from AI models)\n            simulated_detection = {\n                'type': 'weapon_detection',\n                'confidence': 0.9,\n                'bbox': (200, 150, 100, 100),\n                'threat_level': 'HIGH',\n                'description': 'Simulated weapon detection for testing'\n            }\n            \n            # Test threat processing with rules evaluation\n            # Note: Since we don't have actual AI models running, we'll test the rules evaluation directly\n            threat_data = {\n                'zone_id': 'test_entrance',\n                'threat_type': 'weapon_detection',\n                'confidence': 90,\n                'bbox': (200, 150, 100, 100),\n                'camera_id': 'CAM-TEST-01',\n                'threat_level': 'HIGH'\n            }\n            \n            # Test rules evaluation\n            test_result = self.master_coordinator.test_threat_against_rules(threat_data)\n            \n            success = test_result.get('overall_result') == 'TRIGGERED'\n            details = f\"Test result: {test_result.get('overall_result')}, rules triggered: {test_result.get('rules_triggered', 0)}\"\n            \n            self.log_test_result(\"End-to-End Processing\", success, details)\n            return success\n            \n        except Exception as e:\n            self.log_test_result(\"End-to-End Processing\", False, f\"Exception: {str(e)}\")\n            return False\n    \n    def test_zone_and_rule_queries(self) -> bool:\n        \"\"\"Test querying zones and rules for specific cameras\"\"\"\n        try:\n            if not self.master_coordinator:\n                self.log_test_result(\"Zone and Rule Queries\", False, \"Master coordinator not available\")\n                return False\n            \n            camera_id = 'CAM-TEST-01'\n            \n            # Get zones for camera\n            zones = self.master_coordinator.get_geofencing_zones_for_camera(camera_id)\n            \n            # Get rules for camera\n            rules = self.master_coordinator.get_applicable_rules_for_camera(camera_id)\n            \n            success = len(zones) > 0 and len(rules) > 0\n            details = f\"Zones for camera {camera_id}: {len(zones)}, Rules: {len(rules)}\"\n            \n            self.log_test_result(\"Zone and Rule Queries\", success, details)\n            return success\n            \n        except Exception as e:\n            self.log_test_result(\"Zone and Rule Queries\", False, f\"Exception: {str(e)}\")\n            return False\n    \n    def run_all_tests(self) -> Dict[str, any]:\n        \"\"\"Run all integration tests\"\"\"\n        logger.info(\"🚀 Starting Dynamic Rules Engine Integration Tests...\")\n        \n        if not COMPONENTS_AVAILABLE:\n            logger.error(\"❌ Required components not available - cannot run tests\")\n            return {'error': 'Components not available'}\n        \n        # Run tests in sequence\n        tests = [\n            self.test_geofencing_manager_creation,\n            self.test_rules_engine_creation,\n            self.test_configuration_manager,\n            self.test_master_coordinator_integration,\n            self.test_end_to_end_threat_processing,\n            self.test_zone_and_rule_queries\n        ]\n        \n        for test_func in tests:\n            try:\n                test_func()\n            except Exception as e:\n                logger.error(f\"❌ Test {test_func.__name__} failed with exception: {e}\")\n                self.log_test_result(test_func.__name__, False, f\"Exception: {str(e)}\")\n        \n        # Generate summary\n        success_rate = (self.test_results['passed_tests'] / max(self.test_results['total_tests'], 1)) * 100\n        \n        summary = {\n            **self.test_results,\n            'success_rate': success_rate,\n            'overall_status': 'PASS' if success_rate >= 80 else 'FAIL'\n        }\n        \n        logger.info(f\"🏁 Test Summary: {self.test_results['passed_tests']}/{self.test_results['total_tests']} passed ({success_rate:.1f}%)\")\n        \n        return summary\n\ndef main():\n    \"\"\"Main test execution\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"APEX AI DYNAMIC RULES ENGINE INTEGRATION TEST\")\n    print(\"=\" * 60)\n    \n    if not COMPONENTS_AVAILABLE:\n        print(\"❌ Cannot run tests - required components not available\")\n        return\n    \n    # Create and run test suite\n    test_suite = DynamicRulesEngineIntegrationTest()\n    results = test_suite.run_all_tests()\n    \n    # Print detailed results\n    print(\"\\n📊 DETAILED TEST RESULTS:\")\n    print(\"-\" * 40)\n    \n    for test_detail in results['test_details']:\n        print(f\"{test_detail['status']} {test_detail['test_name']}\")\n        if test_detail['details']:\n            print(f\"   📝 {test_detail['details']}\")\n    \n    print(f\"\\n🎯 FINAL RESULTS:\")\n    print(f\"   Total Tests: {results['total_tests']}\")\n    print(f\"   Passed: {results['passed_tests']}\")\n    print(f\"   Failed: {results['failed_tests']}\")\n    print(f\"   Success Rate: {results['success_rate']:.1f}%\")\n    print(f\"   Overall Status: {results['overall_status']}\")\n    \n    if results['overall_status'] == 'PASS':\n        print(\"\\n🎉 DYNAMIC RULES ENGINE INTEGRATION SUCCESSFUL!\")\n        print(\"   Your geofencing and rules engine are working correctly.\")\n    else:\n        print(\"\\n⚠️ INTEGRATION ISSUES DETECTED\")\n        print(\"   Please review the failed tests above.\")\n    \n    print(\"\\n\" + \"=\" * 60)\n\nif __name__ == \"__main__\":\n    main()\n